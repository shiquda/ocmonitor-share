# OpenCode Monitor Changes Report

## Date: February 2, 2026

---

## Summary

This report documents two major feature enhancements to the OpenCode Monitor application:

1. **Dashboard Rate Display Enhancement** - Changed from total tokens per minute to output tokens per second using a rolling 5-minute window
2. **Model Breakdown Speed Column** - Added average output speed tracking to the model usage report

---

## 1. Dashboard Rate Display Enhancement

### Previous Behavior
The live dashboard displayed a "burn rate" showing **total tokens (input + output + cache) per minute** for the entire session duration. This calculation included:
- All token types (input, output, cache_read, cache_write)
- Session idle time (from session start to current time)
- Single interaction focus

### New Behavior
The dashboard now displays **output tokens per second** using a **5-minute rolling window** approach:
- Only counts **output tokens** (the tokens generated by the AI model)
- Uses a rolling window of the last 5 minutes of activity
- Sums active processing time from all interactions within the window
- Excludes idle time between interactions

### Implementation Details

**File: `services/live_monitor.py`**

**Old calculation:**
```python
def _calculate_burn_rate(self, session: SessionData) -> float:
    total_tokens = session.total_tokens.total
    if session.start_time:
        session_duration = current_time - session.start_time
        duration_minutes = session_duration.total_seconds() / 60
        return total_tokens / duration_minutes
```

**New calculation:**
```python
def _calculate_output_rate(self, session: SessionData) -> float:
    # Calculate the cutoff time (5 minutes ago)
    cutoff_time = datetime.now() - timedelta(minutes=5)
    
    # Filter interactions from the last 5 minutes
    recent_interactions = [f for f in session.files 
                          if f.modification_time >= cutoff_time]
    
    # Sum output tokens and active processing time
    total_output_tokens = sum(f.tokens.output for f in recent_interactions)
    total_duration_ms = sum(f.time_data.duration_ms for f in recent_interactions 
                           if f.time_data and f.time_data.duration_ms)
    
    return total_output_tokens / (total_duration_ms / 1000)  # tokens per second
```

**File: `ui/dashboard.py`**

**UI changes:**
- Updated panel title from "Rate" to "Output Rate"
- Changed display from "tok/min" to "tok/sec"
- Adjusted thresholds for high/medium/low indicators:
  - HIGH: > 167 tok/sec (was > 10,000 tok/min)
  - MEDIUM: > 83 tok/sec (was > 5,000 tok/min)
- Format changed to show 1 decimal place (e.g., "125.5 tok/sec")

### Benefits
1. **Smoother results** - Rolling window averages out individual interaction variations
2. **Excludes idle time** - Only measures actual model processing time
3. **Focuses on output** - Most relevant metric for model performance (generation speed)
4. **More responsive** - Recent activity is reflected immediately, stale data is excluded

---

## 2. Model Breakdown Speed Column

### Overview
Added a new "Speed" column to the `ocmonitor models` command output, showing the average output token generation speed for each model across all historical usage.

### Implementation Details

**File: `models/analytics.py`**

**Model changes:**
```python
class ModelUsageStats(BaseModel):
    # ... existing fields ...
    total_duration_ms: int = Field(default=0, 
                                   description="Total processing time in milliseconds")
    
    @computed_field
    @property
    def avg_output_rate(self) -> float:
        """Calculate average output tokens per second."""
        if self.total_duration_ms <= 0 or self.total_tokens.output == 0:
            return 0.0
        duration_seconds = self.total_duration_ms / 1000
        return self.total_tokens.output / duration_seconds
```

**Aggregation logic in `create_model_breakdown()`:**
```python
model_data = defaultdict(lambda: {
    'tokens': TokenUsage(),
    'sessions': set(),
    'interactions': 0,
    'cost': Decimal('0.0'),
    'duration_ms': 0,  # NEW: Track total processing time
    'first_used': None,
    'last_used': None
})

# When processing each file:
for file in model_files:
    # ... existing token and cost aggregation ...
    if file.time_data and file.time_data.duration_ms:
        model_stats['duration_ms'] += file.time_data.duration_ms
```

**File: `ui/tables.py`**

Added "Speed" column to the model breakdown table:
```python
table.add_column("Speed", justify="right", style="cyan")

# In the row generation:
speed = model.avg_output_rate
if speed == 0:
    speed_text = "-"
else:
    speed_text = f"{speed:.1f} t/s"
```

**File: `services/report_generator.py`**

Updated export formats to include speed data:
- **JSON export**: Added `avg_output_rate` field to each model entry
- **CSV export**: Added `avg_output_rate` column

**File: `services/dashboard_generator.py`**

Updated `_model_to_dict()` to include speed in dashboard data exports.

### Display Format

The table now shows:
```
Model                        Sessions   Interactions   Input Tokens   Output Tokens   Total Tokens   Cost    Cost %   Speed
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
claude-opus-4.5-20251101     5          25             15,000         45,000          60,000         $2.50   45.0%    125.5 t/s
gpt-4o-2024-08-06            3          12             8,000          22,000          30,000         $1.80   32.0%    95.3 t/s
claude-sonnet-4-20250514     2          8              5,000          12,000          17,000         $1.30   23.0%    78.2 t/s
```

### Benefits
1. **Model comparison** - Easily compare token generation speeds across different models
2. **Historical view** - Average across all sessions, not just recent ones
3. **Consistent metric** - Uses same "output tokens per second" as dashboard
4. **Export support** - Available in both JSON and CSV export formats

---

## Files Modified

1. `ocmonitor/services/live_monitor.py` - Dashboard rate calculation
2. `ocmonitor/ui/dashboard.py` - Dashboard UI display
3. `ocmonitor/models/analytics.py` - Model stats data model and aggregation
4. `ocmonitor/ui/tables.py` - Model breakdown table display
5. `ocmonitor/services/report_generator.py` - JSON/CSV export formats
6. `ocmonitor/services/dashboard_generator.py` - Dashboard data export

---

## Testing

All changes have been tested with:
- Basic import and initialization tests
- Configuration loading tests
- Service initialization tests
- CLI help command verification

**Test results**: ✅ 4/4 tests passed

---

## Migration Notes

These changes are backward compatible:
- No changes to existing data structures in storage
- Only affects display and export formats
- Existing session data will work immediately
- Speed calculations will populate as new data is processed

---

## Future Enhancements (Optional)

Potential improvements for future iterations:

1. **Dashboard**:
   - Configurable time window (currently hardcoded to 5 minutes)
   - Visual trend indicator (up/down arrow vs previous window)
   - Configurable rate unit (tok/sec vs tok/min)

2. **Model Breakdown**:
   - Sort by speed column
   - Filter by minimum interaction count for more accurate averages
   - Show speed trend over time (faster/slower than previous period)
   - Add median speed option to reduce outlier impact

---

## Conclusion

These enhancements provide users with more meaningful performance metrics:

- **Dashboard**: Real-time view of current generation speed with smoothing
- **Models Report**: Historical comparison of model performance across all usage

Both metrics focus on **output tokens per second** as the primary indicator of model responsiveness and throughput.

---

*Report generated by OpenCode Monitor development team*
